---
title: "Process sequence data"
author: "John Blischak"
date: 2015-04-11
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

All processing scripts were run from the data directory.

```bash
cd /mnt/gluster/data/internal_supp/singleCellSeq
```

The raw fastq files were write-protected to avoid accidental deletion.

```bash
chmod uga=r fastq/*fastq.gz
```

## Run FastQC

```bash
submit-array.sh run-fastqc.sh 2g fastq/*fastq.gz
```

```bash
ls fastq/ | wc -l
grep -w success ~/log/run-fastqc.sh/* | wc -l
grep -w failure ~/log/run-fastqc.sh/* | wc -l
```

## Trim UMI

```bash
submit-array.sh trim.sh 2g fastq/*fastq.gz
```

To confirm that the jobs ran successfully:

```bash
ls trim/*fastq.gz | wc -l
grep -w success ~/log/trim.sh/* | wc -l
grep -w failure ~/log/trim.sh/* | wc -l
```

To re-run failed jobs, I re-ran the original command.
If the output file already exists, the code is not run and "success" is not echo'd to the log file.

## Quality trim 3' end of reads

```bash
submit-array.sh sickle.sh 2g trim/*fastq.gz
```

To confirm that the jobs ran successfully:

```bash
ls sickle/*fastq.gz | wc -l
grep -w success ~/log/sickle.sh/* | wc -l
grep -w failure ~/log/sickle.sh/* | wc -l
```

## Assess sequence characteristics

```bash
submit-array.sh qc.sh 2g fastq/*fastq.gz trim/*fastq.gz sickle/*fastq.gz
```

```bash
ls seqqs/*_len.txt | wc -l
grep -w success ~/log/qc.sh/* | wc -l
grep -w failure ~/log/qc.sh/* | wc -l
```

## Map to genome

```bash
submit-array.sh map-subread.sh 8g trim/*fastq.gz sickle/*fastq.gz
```

```bash
ls bam/*bam | wc -l
grep -w success ~/log/map-subread.sh/* | wc -l
grep -w failure ~/log/map-subread.sh/* | wc -l
```

## Process bam files

*  Sort bam
*  Index bam

```bash
submit-array.sh process-bam.sh 8g bam/*bam
```

```bash
ls bam-processed/*bam | wc -l
grep -w success ~/log/process-bam.sh/* | wc -l
grep -w failure ~/log/process-bam.sh/* | wc -l
```

Check for the presence of intermediate files output during sorting.

```bash
ls bam-processed/*sorted*0*bam
```

## Remove duplicate UMIs

```bash
submit-array.sh rmdup-umi.sh 2g bam-processed/*bam
```

```bash
ls bam-rmdup-umi/*bam | wc -l
grep -w success ~/log/rmdup-umi.sh/* | wc -l
grep -w failure ~/log/rmdup-umi.sh/* | wc -l
```

## Count reads per gene

```bash
submit-array.sh count-reads-per-gene.sh 2g bam-processed/*bam bam-rmdup-umi/*bam
```

```bash
ls counts/*genecounts.txt | wc -l
grep -w success ~/log/count-reads-per-gene.sh/* | wc -l
grep -w failure ~/log/count-reads-per-gene.sh/* | wc -l
```

## Gather total counts

The total number of reads at each stage of the processing pipeline.

```bash
gather-total-counts.py > total-counts.txt
```

## Gather summary counts

The classification of reads by featureCounts.

```bash
gather-summary-counts.py > summary-counts.txt
```

## Gather gene counts

The counts for each gene for each sequencing lane.

```bash
gather-gene-counts.py > gene-counts.txt
```

## Process at the sample level

All the processing performed above is done at the level of the sequencing lane.
Thus when removing reads that have the same UMI and start position, this is done at the level of lane instead of the whole sample.

To get true molecule counts, need to run `umitools rmdup` on all the reads for a given sample.
Only using sickle trimmed data.
Starting with the sorted bam files, merge the files per sample.


```bash
# From head node
cd $ssd
mkdir -p bam-combined
mkdir -p ~/log/combine.sh
for IND in 19098 19101 19239
do
  for BATCH in {1..3}
  do
    for ROW in {A..H}
    do
      for COL in {1..12}
      do
        ID=`printf "%s.%s.%s%02d\n" $IND $BATCH $ROW $COL`
        echo $ID
        echo "samtools merge bam-combined/$ID.trim.sickle.sorted.bam bam-processed/$ID*trim.sickle.sorted.bam" | qsub -l h_vmem=4g -N $ROW$COL.$IND.$BATCH.combine -cwd -o ~/log/combine.sh -j y -V
      done
    done
  done
done
```

Then index each merged sample:

```bash
mkdir -p ~/log/combine2.sh
for IND in 19098 19101 19239
do
  for BATCH in {1..3}
  do
    for ROW in {A..H}
    do
      for COL in {1..12}
      do
        ID=`printf "%s.%s.%s%02d\n" $IND $BATCH $ROW $COL`
        echo $ID
        echo "samtools index bam-combined/$ID.trim.sickle.sorted.bam" | qsub -l h_vmem=2g -N $ROW$COL.$IND.$BATCH.index -cwd -o ~/log/combine2.sh -j y -V
      done
    done
  done
done
```

Now can run `rmdup umitools` and `featureCounts` as above:

```bash
submit-array.sh rmdup-umi.sh 2g bam-combined/*bam
```

Since do not want to re-run all the samples, need to change the name so that it is clear it is a combined sample.

```bash
rename sorted.bam sorted.combined.bam *trim.sickle.sorted.bam
rename sorted.bam.bai sorted.combined.bam.bai *trim.sickle.sorted.bam.bai
```

Now count the number of reads per gene for both the molecules and the reads:

```bash
submit-array.sh count-reads-per-gene.sh 2g bam-rmdup-umi/*combined.rmdup.bam bam-combined/*bam
```

Now need to do the same for the bulk samples:

Merge bulk samples.

```bash
# From head node
cd $ssd
mkdir ~/log/bulk
for IND in 19098 19101 19239
do
  for BATCH in {1..3}
  do
    ID=$IND.$BATCH.bulk
    echo $ID
    echo "samtools merge bam-combined/$ID.trim.sickle.sorted.combined.bam bam-processed/$ID*trim.sickle.sorted.bam" | qsub -l h_vmem=8g -N bulk.$IND.$BATCH.combine -cwd -o ~/log/bulk -j y -V
  done
done
```

Index bulk samples.

```bash
# From head node
cd $ssd
mkdir -p ~/log/bulk
for IND in 19098 19101 19239
do
  for BATCH in {1..3}
  do
    ID=$IND.$BATCH.bulk
    echo $ID
    echo "samtools index bam-combined/$ID.trim.sickle.sorted.combined.bam" | qsub -l h_vmem=8g -N bulk.$IND.$BATCH.index -cwd -o ~/log/bulk -j y -V
  done
done
```

Remove duplicates and count reads per gene.

```bash
submit-array.sh rmdup-umi.sh 8g bam-combined/*bulk*bam
```

```bash
submit-array.sh count-reads-per-gene.sh 2g bam-rmdup-umi/*bulk*combined.rmdup.bam bam-combined/*bulk*bam
```
